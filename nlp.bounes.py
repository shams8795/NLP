# -*- coding: utf-8 -*-
"""Untitled68.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XZZS8YqW4dgU_OyjU9Zgm3NLBcf1TLKQ
"""

!pip install -U datasets

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import torch
from datasets import load_dataset
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

dataset = load_dataset("lex_glue", "ecthr_a")  # Ø£Ùˆ "ledgar"/"scotus" Ø­Ø³Ø¨ Ø§Ø®ØªÙŠØ§Ø±Ùƒ

from transformers import pipeline

classifier = pipeline("text-generation", model="gpt2")

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
from datasets import load_dataset
dataset = load_dataset("lex_glue", "ledgar")  # Ø£Ùˆ "scotus"

# 2. Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù†ØµÙŠØ©
example_text = dataset["test"][0]["text"]  # Ø£Ùˆ "train" Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† "test" Ù„Ùˆ Ø­Ø¨ÙŠØªÙŠ

# 3. ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ
from transformers import pipeline
classifier = pipeline("text-generation", model="gpt2")

# 4. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª
prompt = (
    "Given the following legal document, classify it into one of the categories "
    "(e.g., Contract, Memo, Agreement):\n\n"
    f"Document:\n{example_text}\n\nLabel:"
)

# 5. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØµÙ†ÙŠÙ
output = classifier(prompt, max_new_tokens=15)
print(output[0]["generated_text"])

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True)

tokenized_dataset = dataset.map(tokenize, batched=True)

!pip install -U transformers

# Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù„Ù„ÙŠØ¨Ù„Ø§Øª
print("âœ… Labels summary:")
print("Train labels:", torch.unique(train_dataset["labels"]))
print("Max label:", torch.max(train_dataset["labels"]))
print("Min label:", torch.min(train_dataset["labels"]))
print("num_labels to be used:", int(torch.max(train_dataset["labels"]).item()) + 1)

# ÙÙ„ØªØ±Ø© test: Ù†Ø®Ù„ÙŠ Ø¨Ø³ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù„ÙŠ labelÙ‡Ø§ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
valid_labels = set(all_labels)
small_test_dataset = small_test_dataset.filter(lambda example: example["label"] in valid_labels)

test_labels = label_encoder.transform(small_test_dataset["label"])
small_test_dataset = small_test_dataset.remove_columns("label")
small_test_dataset = small_test_dataset.add_column("label", test_labels)

# âœ… 1. ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª (Ù„Ùˆ Ù…Ø´ Ù…ØªÙˆÙØ±Ø©)
!pip install datasets scikit-learn

# âœ… 2. Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
from datasets import load_dataset
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.pipeline import Pipeline

# âœ… 3. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø©)
dataset = load_dataset("lex_glue", "ledgar")
train_texts = dataset["train"]["text"][:1000]
train_labels = dataset["train"]["label"][:1000]
test_texts = dataset["test"]["text"][:200]
test_labels = dataset["test"]["label"][:200]

# âœ… 4. ØªØ±Ù…ÙŠØ² Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ù„ØªÙƒÙˆÙ† Ø£Ø±Ù‚Ø§Ù… Ù…ØªØ³Ù„Ø³Ù„Ø©
le = LabelEncoder()
train_labels_enc = le.fit_transform(train_labels)
test_labels_enc = le.transform([lbl for lbl in test_labels if lbl in le.classes_])

# âœ… 5. ØªØµÙÙŠØ© test_texts Ù…Ù† Ø£ÙŠ label ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ
filtered_test_texts = [text for text, label in zip(test_texts, test_labels) if label in le.classes_]

# âœ… 6. Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Logistic Regression Ø¯Ø§Ø®Ù„ pipeline
clf = Pipeline([
    ("tfidf", TfidfVectorizer(max_features=3000)),
    ("logreg", LogisticRegression(max_iter=1000))
])

# âœ… 7. ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
clf.fit(train_texts, train_labels_enc)

# âœ… 8. Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…
preds = clf.predict(filtered_test_texts)
print("\nğŸ“‹ Classification Report (Logistic Regression):")
print(classification_report(test_labels_enc, preds, digits=2))

!pip install datasets transformers scikit-learn

!pip install datasets

from datasets import load_dataset
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
dataset = load_dataset("lex_glue", "ledgar")
texts = dataset["train"]["text"][:1000]
labels = dataset["train"]["label"][:1000]

# 2. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø¨Ø¯ÙˆÙ† stratify)
X_train, X_test, y_train, y_test = train_test_split(
    texts, labels, test_size=0.2, random_state=42
)

# 3. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# 4. ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Naive Bayes": MultinomialNB(),
    "Random Forest": RandomForestClassifier(n_estimators=100)
}

# 5. ØªÙ‚ÙŠÙŠÙ… ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬
for name, model in models.items():
    print(f"ğŸ”¹ {name}")
    model.fit(X_train_tfidf, y_train)
    preds = model.predict(X_test_tfidf)
    print(classification_report(y_test, preds))
    print("="*60)

import matplotlib.pyplot as plt

# Ø§Ù„Ù‚ÙŠÙ… Ù…Ù† ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ
metrics = ['Accuracy', 'Macro Precision', 'Macro Recall', 'Macro F1', 'Weighted Precision', 'Weighted Recall', 'Weighted F1']
values = [0.67, 0.52, 0.57, 0.52, 0.63, 0.67, 0.62]

# Ø±Ø³Ù… Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ
plt.figure(figsize=(10, 6))
bars = plt.bar(metrics, values)
plt.ylim(0, 1)
plt.title("Classification Metrics for Logistic Regression")
plt.ylabel("Score")
plt.xticks(rotation=45)
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f"{yval:.2f}", ha='center', va='bottom')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† (Ù…Ø«Ø§Ù„ØŒ Ø§Ø³ØªØ¨Ø¯Ù„Ù‡Ø§ Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©)
model_names = ["TF-IDF + Logistic", "BERT"]
accuracy = [0.53, 0.67]
precision = [0.41, 0.50]
recall = [0.53, 0.56]
f1 = [0.43, 0.50]

# Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ
x = range(len(model_names))
width = 0.2

plt.figure(figsize=(10, 6))
plt.bar([i - 1.5 * width for i in x], accuracy, width, label="Accuracy")
plt.bar([i - 0.5 * width for i in x], precision, width, label="Precision")
plt.bar([i + 0.5 * width for i in x], recall, width, label="Recall")
plt.bar([i + 1.5 * width for i in x], f1, width, label="F1 Score")

plt.xticks(x, model_names)
plt.ylabel("Score")
plt.ylim(0, 1)
plt.title("Model Comparison")
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()